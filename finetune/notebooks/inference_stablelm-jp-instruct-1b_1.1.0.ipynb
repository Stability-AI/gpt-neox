{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fe2905fb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, random_split\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import datasets as D\n",
    "from datasets import load_dataset\n",
    "import transformers as T\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "645d834b",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e2e2807e",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_PROMPT = \"\"\"以下は、タスクを説明する指示と、文脈のある入力の組み合わせです。要求を適切に満たす応答を書きなさい。\n",
    "\n",
    "### 指示: \n",
    "{instruction}\n",
    "\n",
    "### 入力: \n",
    "{input}\n",
    "\n",
    "### 応答: \"\"\"\n",
    "\n",
    "NO_INPUT_PROMPT = \"\"\"以下は、タスクを説明する指示と、文脈のある入力の組み合わせです。要求を適切に満たす応答を書きなさい。\n",
    "\n",
    "### 指示: \n",
    "{instruction}\n",
    "\n",
    "### 応答: \"\"\"\n",
    "\n",
    "\n",
    "def postprocess_output(output):\n",
    "    output = output\\\n",
    "        .split('### 応答:')[1]\\\n",
    "        .split('###')[0]\\\n",
    "        .split('##')[0]\\\n",
    "        .lstrip(tokenizer.bos_token)\\\n",
    "        .rstrip(tokenizer.eos_token)\\\n",
    "        .replace(\"###\", \"\")\\\n",
    "        .strip()\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b8b969",
   "metadata": {},
   "source": [
    "## 3rd trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "031791a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class args:\n",
    "    tokenizer_name_or_path: str = \"/PATH/TO/tokenizer\"\n",
    "    model_name_or_path: str = \"/PATH/TO/stablelm-jp-instruct-1b_1.3.0/checkpoint-19113/\"\n",
    "    cache_dir = \"/PATH/TO/.cache\"\n",
    "        \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9964a41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name_or_path, use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "44ee0fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(args.model_name_or_path)\n",
    "model = model.to(device)\n",
    "print(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b2386cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bos_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "日本は美しい自然や歴史的な建造物など多くの魅力があります。また、日本には素晴らしい食べ物もあります！\n"
     ]
    }
   ],
   "source": [
    "instruction = \"日本のおすすめの観光スポットは？\"\n",
    "input_text = NO_INPUT_PROMPT.format(instruction=instruction)\n",
    "token_ids = tokenizer.encode(input_text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(\n",
    "        token_ids.to(model.device),\n",
    "        min_length=32,\n",
    "        max_length=128,\n",
    "        temperature=0.1,\n",
    "        do_sample=True,\n",
    "        # no_repeat_ngram_size=3,\n",
    "        repetition_penalty=1.2,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        bad_words_ids=[[tokenizer.unk_token_id]]\n",
    "    )\n",
    "\n",
    "output = tokenizer.decode(output_ids.tolist()[0])\n",
    "print(postprocess_output(output).split('応答:')[-1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "876c8e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bos_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "東京\n"
     ]
    }
   ],
   "source": [
    "instruction = \"日本の首都はどこ？\"\n",
    "input_text = NO_INPUT_PROMPT.format(instruction=instruction)\n",
    "token_ids = tokenizer.encode(input_text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(\n",
    "        token_ids.to(model.device),\n",
    "        min_length=32,\n",
    "        max_length=128,\n",
    "        temperature=0.1,\n",
    "        do_sample=True,\n",
    "        # no_repeat_ngram_size=3,\n",
    "        repetition_penalty=1.2,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        bad_words_ids=[[tokenizer.unk_token_id]]\n",
    "    )\n",
    "\n",
    "output = tokenizer.decode(output_ids.tolist()[0])\n",
    "print(postprocess_output(output).split('応答:')[-1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "51058c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bos_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "東京\n"
     ]
    }
   ],
   "source": [
    "instruction = \"日本の首都は？\"\n",
    "input_text = NO_INPUT_PROMPT.format(instruction=instruction)\n",
    "token_ids = tokenizer.encode(input_text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(\n",
    "        token_ids.to(model.device),\n",
    "        min_length=32,\n",
    "        max_length=128,\n",
    "        temperature=0.1,\n",
    "        do_sample=True,\n",
    "        # no_repeat_ngram_size=3,\n",
    "        repetition_penalty=1.2,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        bad_words_ids=[[tokenizer.unk_token_id]]\n",
    "    )\n",
    "\n",
    "output = tokenizer.decode(output_ids.tolist()[0])\n",
    "print(postprocess_output(output).split('応答:')[-1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5482502a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bos_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "現在の大統領であるドナルド・トランプ氏は、2019年4月3日に就任しました。\n"
     ]
    }
   ],
   "source": [
    "instruction = \"現在の日本の首相は？\"\n",
    "input_text = NO_INPUT_PROMPT.format(instruction=instruction)\n",
    "token_ids = tokenizer.encode(input_text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(\n",
    "        token_ids.to(model.device),\n",
    "        min_length=32,\n",
    "        max_length=128,\n",
    "        temperature=0.1,\n",
    "        do_sample=True,\n",
    "        # no_repeat_ngram_size=3,\n",
    "        repetition_penalty=1.2,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        bad_words_ids=[[tokenizer.unk_token_id]]\n",
    "    )\n",
    "\n",
    "output = tokenizer.decode(output_ids.tolist()[0])\n",
    "print(postprocess_output(output).split('応答:')[-1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "76d87931",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bos_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "現在の日本国の首相は安倍晋三です。\n"
     ]
    }
   ],
   "source": [
    "instruction = \"現在の日本の内閣総理大臣は？\"\n",
    "input_text = NO_INPUT_PROMPT.format(instruction=instruction)\n",
    "token_ids = tokenizer.encode(input_text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(\n",
    "        token_ids.to(model.device),\n",
    "        min_length=32,\n",
    "        max_length=128,\n",
    "        temperature=0.1,\n",
    "        do_sample=True,\n",
    "        # no_repeat_ngram_size=3,\n",
    "        repetition_penalty=1.2,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        bad_words_ids=[[tokenizer.unk_token_id]]\n",
    "    )\n",
    "\n",
    "output = tokenizer.decode(output_ids.tolist()[0])\n",
    "print(postprocess_output(output).split('応答:')[-1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8bbe9030",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bos_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "私はK-POPのアーティストが好きで、特にBTSやEXOなどのグループが彼らのパフォーマンスに魅了されています！\n"
     ]
    }
   ],
   "source": [
    "instruction = \"好きなアイドルを教えて。\"\n",
    "input_text = NO_INPUT_PROMPT.format(instruction=instruction)\n",
    "token_ids = tokenizer.encode(input_text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(\n",
    "        token_ids.to(model.device),\n",
    "        min_length=32,\n",
    "        max_length=128,\n",
    "        temperature=0.1,\n",
    "        do_sample=True,\n",
    "        # no_repeat_ngram_size=3,\n",
    "        repetition_penalty=1.2,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        bad_words_ids=[[tokenizer.unk_token_id]]\n",
    "    )\n",
    "\n",
    "output = tokenizer.decode(output_ids.tolist()[0])\n",
    "print(postprocess_output(output).split('応答:')[-1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ef1c35da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bos_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "トヨタは1927年に設立されました。\n"
     ]
    }
   ],
   "source": [
    "instruction = \"トヨタ自動車は何年設立ですか？\"\n",
    "input_text = NO_INPUT_PROMPT.format(instruction=instruction)\n",
    "token_ids = tokenizer.encode(input_text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(\n",
    "        token_ids.to(model.device),\n",
    "        min_length=32,\n",
    "        max_length=128,\n",
    "        temperature=0.1,\n",
    "        do_sample=True,\n",
    "        # no_repeat_ngram_size=3,\n",
    "        repetition_penalty=1.2,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        bad_words_ids=[[tokenizer.unk_token_id]]\n",
    "    )\n",
    "\n",
    "output = tokenizer.decode(output_ids.tolist()[0])\n",
    "print(postprocess_output(output).split('応答:')[-1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e65ecc9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bos_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "いいえ、運転免許証やパスポートなどの身分証明書を提示する必要があります。\n"
     ]
    }
   ],
   "source": [
    "instruction = \"マイナンバーカードの受け取りは免許書を持って行けばいいですか？\"\n",
    "input_text = NO_INPUT_PROMPT.format(instruction=instruction)\n",
    "token_ids = tokenizer.encode(input_text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(\n",
    "        token_ids.to(model.device),\n",
    "        min_length=32,\n",
    "        max_length=128,\n",
    "        temperature=0.1,\n",
    "        do_sample=True,\n",
    "        # no_repeat_ngram_size=3,\n",
    "        repetition_penalty=1.2,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        bad_words_ids=[[tokenizer.unk_token_id]]\n",
    "    )\n",
    "\n",
    "output = tokenizer.decode(output_ids.tolist()[0])\n",
    "print(postprocess_output(output).split('応答:')[-1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ce083709",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bos_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "地球温暖化の原因は主に二酸化炭素の増加によるものです。これは大気中に放出される化学物質によって引き起こされます。この化学物質にはメタンや一酸化窒素などの炭酸塩が含まれています。これらの化合物は太陽からの熱を吸収し、地球の気温を下げるために使用されています。\n"
     ]
    }
   ],
   "source": [
    "instruction = \"地球温暖化とはなんですか？\"\n",
    "input_text = NO_INPUT_PROMPT.format(instruction=instruction)\n",
    "token_ids = tokenizer.encode(input_text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(\n",
    "        token_ids.to(model.device),\n",
    "        min_length=32,\n",
    "        max_length=128,\n",
    "        temperature=0.1,\n",
    "        do_sample=True,\n",
    "        # no_repeat_ngram_size=3,\n",
    "        repetition_penalty=1.2,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        bad_words_ids=[[tokenizer.unk_token_id]]\n",
    "    )\n",
    "\n",
    "output = tokenizer.decode(output_ids.tolist()[0])\n",
    "print(postprocess_output(output).split('応答:')[-1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff91954",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6b2791",
   "metadata": {},
   "outputs": [],
   "source": [
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00ef35c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class args:\n",
    "    tokenizer_name_or_path: str = \"/PATH/TO/tokenizer\"\n",
    "    model_name_or_path: str = \"/PATH/TO/stablelm-jp-instruct-1b_1.1.0/checkpoint-12742/\"\n",
    "    cache_dir = \"/PATH/TO/.cache\"\n",
    "        \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "109f7b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name_or_path, use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ab177ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(args.model_name_or_path)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12935238",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bos_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "日本には多くの見どころがありますが、最も人気があるのは富士山や京都などの大都市でしょう。また、東京ディズニーランドのようなテーマパークも人気があります。\n"
     ]
    }
   ],
   "source": [
    "instruction = \"日本のおすすめの観光スポットは？\"\n",
    "input_text = NO_INPUT_PROMPT.format(instruction=instruction)\n",
    "token_ids = tokenizer.encode(input_text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(\n",
    "        token_ids.to(model.device),\n",
    "        min_length=32,\n",
    "        max_length=128,\n",
    "        temperature=0.1,\n",
    "        do_sample=True,\n",
    "        # no_repeat_ngram_size=3,\n",
    "        repetition_penalty=1.2,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        bad_words_ids=[[tokenizer.unk_token_id]]\n",
    "    )\n",
    "\n",
    "output = tokenizer.decode(output_ids.tolist()[0])\n",
    "print(postprocess_output(output).split('応答:')[-1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f2be5bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bos_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "東京\n"
     ]
    }
   ],
   "source": [
    "instruction = \"日本の首都は？\"\n",
    "input_text = NO_INPUT_PROMPT.format(instruction=instruction)\n",
    "token_ids = tokenizer.encode(input_text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(\n",
    "        token_ids.to(model.device),\n",
    "        min_length=32,\n",
    "        max_length=128,\n",
    "        temperature=0.1,\n",
    "        do_sample=True,\n",
    "        # no_repeat_ngram_size=3,\n",
    "        repetition_penalty=1.2,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        bad_words_ids=[[tokenizer.unk_token_id]]\n",
    "    )\n",
    "\n",
    "output = tokenizer.decode(output_ids.tolist()[0])\n",
    "print(postprocess_output(output).split('応答:')[-1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "223eb820",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bos_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "安倍晋三」という名前の首相がいます。\n"
     ]
    }
   ],
   "source": [
    "instruction = \"現在の日本の首相は？\"\n",
    "input_text = NO_INPUT_PROMPT.format(instruction=instruction)\n",
    "token_ids = tokenizer.encode(input_text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(\n",
    "        token_ids.to(model.device),\n",
    "        min_length=32,\n",
    "        max_length=128,\n",
    "        temperature=0.1,\n",
    "        do_sample=True,\n",
    "        # no_repeat_ngram_size=3,\n",
    "        repetition_penalty=1.2,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        bad_words_ids=[[tokenizer.unk_token_id]]\n",
    "    )\n",
    "\n",
    "output = tokenizer.decode(output_ids.tolist()[0])\n",
    "print(postprocess_output(output).split('応答:')[-1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab24a325",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bos_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "現在の日本国の首相は安倍晋三です。\n"
     ]
    }
   ],
   "source": [
    "instruction = \"現在の日本の内閣総理大臣は？\"\n",
    "input_text = NO_INPUT_PROMPT.format(instruction=instruction)\n",
    "token_ids = tokenizer.encode(input_text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(\n",
    "        token_ids.to(model.device),\n",
    "        min_length=32,\n",
    "        max_length=128,\n",
    "        temperature=0.1,\n",
    "        do_sample=True,\n",
    "        # no_repeat_ngram_size=3,\n",
    "        repetition_penalty=1.2,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        bad_words_ids=[[tokenizer.unk_token_id]]\n",
    "    )\n",
    "\n",
    "output = tokenizer.decode(output_ids.tolist()[0])\n",
    "print(postprocess_output(output).split('応答:')[-1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db74c396",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bos_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "現在の日本国の首相は安倍晋三です。\n"
     ]
    }
   ],
   "source": [
    "instruction = \"現在の日本の内閣総理大臣は？\"\n",
    "input_text = NO_INPUT_PROMPT.format(instruction=instruction)\n",
    "token_ids = tokenizer.encode(input_text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(\n",
    "        token_ids.to(model.device),\n",
    "        min_length=32,\n",
    "        max_length=128,\n",
    "        temperature=0.1,\n",
    "        do_sample=True,\n",
    "        # no_repeat_ngram_size=3,\n",
    "        repetition_penalty=1.2,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        bad_words_ids=[[tokenizer.unk_token_id]]\n",
    "    )\n",
    "\n",
    "output = tokenizer.decode(output_ids.tolist()[0])\n",
    "print(postprocess_output(output).split('応答:')[-1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c877d792",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bos_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "明日は晴れで気温が20度を超えるでしょう！\n"
     ]
    }
   ],
   "source": [
    "instruction = \"明日の天気は？\"\n",
    "input_text = NO_INPUT_PROMPT.format(instruction=instruction)\n",
    "token_ids = tokenizer.encode(input_text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(\n",
    "        token_ids.to(model.device),\n",
    "        min_length=32,\n",
    "        max_length=128,\n",
    "        temperature=0.1,\n",
    "        do_sample=True,\n",
    "        # no_repeat_ngram_size=3,\n",
    "        repetition_penalty=1.2,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        bad_words_ids=[[tokenizer.unk_token_id]]\n",
    "    )\n",
    "\n",
    "output = tokenizer.decode(output_ids.tolist()[0])\n",
    "print(postprocess_output(output).split('応答:')[-1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7194c011",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bos_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "私は最近、日本のアイドルグループであるAKB48のファンになりました。彼らのパフォーマンスは本当に素晴らしく、いつも楽しませてくれます！\n"
     ]
    }
   ],
   "source": [
    "instruction = \"好きなアイドルを教えて。\"\n",
    "input_text = NO_INPUT_PROMPT.format(instruction=instruction)\n",
    "token_ids = tokenizer.encode(input_text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(\n",
    "        token_ids.to(model.device),\n",
    "        min_length=32,\n",
    "        max_length=128,\n",
    "        temperature=0.1,\n",
    "        do_sample=True,\n",
    "        # no_repeat_ngram_size=3,\n",
    "        repetition_penalty=1.2,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        bad_words_ids=[[tokenizer.unk_token_id]]\n",
    "    )\n",
    "\n",
    "output = tokenizer.decode(output_ids.tolist()[0])\n",
    "print(postprocess_output(output).split('応答:')[-1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3724e0d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bos_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "私は最近、日本のアイドルグループであるKis-My-Ft2の二階堂高嗣さんが好きです！彼はとてもクールで知的で、常に新しいことに挑戦し続けています。彼のパフォーマンスはいつも新鮮でパワフルであり、観客が彼らのパフォーマンスを見るたびに興奮しています。\n"
     ]
    }
   ],
   "source": [
    "instruction = \"好きなアイドルを教えて。\"\n",
    "input_text = NO_INPUT_PROMPT.format(instruction=instruction)\n",
    "token_ids = tokenizer.encode(input_text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(\n",
    "        token_ids.to(model.device),\n",
    "        min_length=32,\n",
    "        max_length=128,\n",
    "        temperature=0.1,\n",
    "        do_sample=True,\n",
    "        # no_repeat_ngram_size=3,\n",
    "        repetition_penalty=1.2,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        bad_words_ids=[[tokenizer.unk_token_id]]\n",
    "    )\n",
    "\n",
    "output = tokenizer.decode(output_ids.tolist()[0])\n",
    "print(postprocess_output(output).split('応答:')[-1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "17abb68a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bos_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "与えられた数の2乗が10になるようにするにはどうすればいいですか？\n"
     ]
    }
   ],
   "source": [
    "instruction = \"数学の問題を作ってください。\"\n",
    "input_text = NO_INPUT_PROMPT.format(instruction=instruction)\n",
    "token_ids = tokenizer.encode(input_text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(\n",
    "        token_ids.to(model.device),\n",
    "        min_length=32,\n",
    "        max_length=128,\n",
    "        temperature=0.1,\n",
    "        do_sample=True,\n",
    "        # no_repeat_ngram_size=3,\n",
    "        repetition_penalty=1.2,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        bad_words_ids=[[tokenizer.unk_token_id]]\n",
    "    )\n",
    "\n",
    "output = tokenizer.decode(output_ids.tolist()[0])\n",
    "print(postprocess_output(output).split('応答:')[-1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dbc94414",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bos_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "与えられた数の2乗が10進数で表されるかどうかを確認してください。\n"
     ]
    }
   ],
   "source": [
    "instruction = \"数学の問題を作ってください。\"\n",
    "input_text = NO_INPUT_PROMPT.format(instruction=instruction)\n",
    "token_ids = tokenizer.encode(input_text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(\n",
    "        token_ids.to(model.device),\n",
    "        min_length=32,\n",
    "        max_length=128,\n",
    "        temperature=0.1,\n",
    "        do_sample=True,\n",
    "        # no_repeat_ngram_size=3,\n",
    "        repetition_penalty=1.2,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        bad_words_ids=[[tokenizer.unk_token_id]]\n",
    "    )\n",
    "\n",
    "output = tokenizer.decode(output_ids.tolist()[0])\n",
    "print(postprocess_output(output).split('応答:')[-1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "51ff970e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bos_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "instruction = \"横須賀はどんなところ？\"\n",
    "input_text = NO_INPUT_PROMPT.format(instruction=instruction)\n",
    "token_ids = tokenizer.encode(input_text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(\n",
    "        token_ids.to(model.device),\n",
    "        min_length=32,\n",
    "        max_length=128,\n",
    "        temperature=0.1,\n",
    "        do_sample=True,\n",
    "        no_repeat_ngram_size=3,\n",
    "        repetition_penalty=1.2,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        bad_words_ids=[[tokenizer.unk_token_id]]\n",
    "    )\n",
    "\n",
    "output = tokenizer.decode(output_ids.tolist()[0])\n",
    "print(postprocess_output(output).split('応答:')[-1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ac7335ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bos_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "instruction = \"人工知能とはどのような技術ですか？\"\n",
    "input_text = NO_INPUT_PROMPT.format(instruction=instruction)\n",
    "token_ids = tokenizer.encode(input_text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(\n",
    "        token_ids.to(model.device),\n",
    "        min_length=32,\n",
    "        max_length=128,\n",
    "        temperature=0.1,\n",
    "        do_sample=True,\n",
    "        no_repeat_ngram_size=3,\n",
    "        repetition_penalty=1.2,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        bad_words_ids=[[tokenizer.unk_token_id]]\n",
    "    )\n",
    "\n",
    "output = tokenizer.decode(output_ids.tolist()[0])\n",
    "print(postprocess_output(output).split('応答:')[-1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3ac3ba63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bos_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "instruction = \"アメリカの首都は？\"\n",
    "input_text = NO_INPUT_PROMPT.format(instruction=instruction)\n",
    "token_ids = tokenizer.encode(input_text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(\n",
    "        token_ids.to(model.device),\n",
    "        min_length=32,\n",
    "        max_length=128,\n",
    "        temperature=0.1,\n",
    "        do_sample=True,\n",
    "        no_repeat_ngram_size=3,\n",
    "        repetition_penalty=1.2,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        bad_words_ids=[[tokenizer.unk_token_id]]\n",
    "    )\n",
    "\n",
    "output = tokenizer.decode(output_ids.tolist()[0])\n",
    "print(postprocess_output(output).split('応答:')[-1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05403e12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "20d67abd",
   "metadata": {},
   "source": [
    "## 1st trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "300b775e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class args:\n",
    "    tokenizer_name_or_path: str = \"/PATH/TO/tokenizer\"\n",
    "    model_name_or_path: str = \"/PATH/TO/stablelm-jp-instruct-1b_1.1.0/checkpoint-6371/\"\n",
    "    cache_dir = \"/PATH/TO/.cache\"\n",
    "        \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3bbbd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name_or_path, use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "037c0557",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(args.model_name_or_path)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc674d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bos_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "日本語で「東京」と言います。\n"
     ]
    }
   ],
   "source": [
    "instruction = \"日本の首都は？\"\n",
    "input_text = NO_INPUT_PROMPT.format(instruction=instruction)\n",
    "token_ids = tokenizer.encode(input_text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(\n",
    "        token_ids.to(model.device),\n",
    "        min_length=32,\n",
    "        max_length=128,\n",
    "        temperature=0.1,\n",
    "        do_sample=True,\n",
    "        no_repeat_ngram_size=3,\n",
    "        repetition_penalty=1.2,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        bad_words_ids=[[tokenizer.unk_token_id]]\n",
    "    )\n",
    "\n",
    "output = tokenizer.decode(output_ids.tolist()[0])\n",
    "print(postprocess_output(output).split('応答:')[-1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d46b1a50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bos_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "日本語で「東京」と言います。\n"
     ]
    }
   ],
   "source": [
    "instruction = \"日本の首都はどこですか？\"\n",
    "input_text = NO_INPUT_PROMPT.format(instruction=instruction)\n",
    "token_ids = tokenizer.encode(input_text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(\n",
    "        token_ids.to(model.device),\n",
    "        min_length=32,\n",
    "        max_length=128,\n",
    "        temperature=0.1,\n",
    "        do_sample=True,\n",
    "        no_repeat_ngram_size=3,\n",
    "        repetition_penalty=1.2,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        bad_words_ids=[[tokenizer.unk_token_id]]\n",
    "    )\n",
    "\n",
    "output = tokenizer.decode(output_ids.tolist()[0])\n",
    "print(postprocess_output(output).split('応答:')[-1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b9ee045",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bos_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "instruction = \"健康を維持するための秘訣は何？\"\n",
    "input_text = NO_INPUT_PROMPT.format(instruction=instruction)\n",
    "token_ids = tokenizer.encode(input_text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(\n",
    "        token_ids.to(model.device),\n",
    "        min_length=32,\n",
    "        max_length=128,\n",
    "        temperature=0.1,\n",
    "        do_sample=True,\n",
    "        no_repeat_ngram_size=3,\n",
    "        repetition_penalty=1.2,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        bad_words_ids=[[tokenizer.unk_token_id]]\n",
    "    )\n",
    "\n",
    "output = tokenizer.decode(output_ids.tolist()[0])\n",
    "print(postprocess_output(output).split('応答:')[-1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e375cb68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bos_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "日本語では東京とされていますが、英語ではTokyoと言います。\n"
     ]
    }
   ],
   "source": [
    "instruction = \"日本の首都は？\"\n",
    "input_text = NO_INPUT_PROMPT.format(instruction=instruction)\n",
    "token_ids = tokenizer.encode(input_text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(\n",
    "        token_ids.to(model.device),\n",
    "        min_length=32,\n",
    "        max_length=128,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        no_repeat_ngram_size=3,\n",
    "        repetition_penalty=1.2,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        bad_words_ids=[[tokenizer.unk_token_id]]\n",
    "    )\n",
    "\n",
    "output = tokenizer.decode(output_ids.tolist()[0])\n",
    "print(postprocess_output(output).split('応答:')[-1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0924e706",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bos_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "アメリカでは、「アメリカ合衆国」が州を表す言葉として使用されます。\n"
     ]
    }
   ],
   "source": [
    "instruction = \"アメリカの首都は？\"\n",
    "input_text = NO_INPUT_PROMPT.format(instruction=instruction)\n",
    "token_ids = tokenizer.encode(input_text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(\n",
    "        token_ids.to(model.device),\n",
    "        min_length=32,\n",
    "        max_length=128,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        no_repeat_ngram_size=3,\n",
    "        repetition_penalty=1.2,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        bad_words_ids=[[tokenizer.unk_token_id]]\n",
    "    )\n",
    "\n",
    "output = tokenizer.decode(output_ids.tolist()[0])\n",
    "print(postprocess_output(output).split('応答:')[-1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "54152812",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bos_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "アメリカの大都市には、ニューヨークやワシントンD.C.などがあります。\n"
     ]
    }
   ],
   "source": [
    "instruction = \"アメリカの首都は？\"\n",
    "input_text = NO_INPUT_PROMPT.format(instruction=instruction)\n",
    "token_ids = tokenizer.encode(input_text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(\n",
    "        token_ids.to(model.device),\n",
    "        min_length=32,\n",
    "        max_length=128,\n",
    "        temperature=0.1,\n",
    "        do_sample=True,\n",
    "        no_repeat_ngram_size=3,\n",
    "        repetition_penalty=1.2,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        bad_words_ids=[[tokenizer.unk_token_id]]\n",
    "    )\n",
    "\n",
    "output = tokenizer.decode(output_ids.tolist()[0])\n",
    "print(postprocess_output(output).split('応答:')[-1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7ddb715",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bos_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "例えば、機械学習や深層強化学習などのAIテクノロジーがあります。これらの技術の多くは、人間の脳の神経回路を模倣してデータからパターンを見つけ出し、そのパターンの特徴に基づいてアルゴリズムを作成します。これにより、人間は複雑な問題を解決するためにより効率的に作業することができます。\n"
     ]
    }
   ],
   "source": [
    "instruction = \"人工知能とはどのような技術ですか？\"\n",
    "input_text = NO_INPUT_PROMPT.format(instruction=instruction)\n",
    "token_ids = tokenizer.encode(input_text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(\n",
    "        token_ids.to(model.device),\n",
    "        min_length=32,\n",
    "        max_length=128,\n",
    "        temperature=0.1,\n",
    "        do_sample=True,\n",
    "        no_repeat_ngram_size=3,\n",
    "        repetition_penalty=1.2,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        bad_words_ids=[[tokenizer.unk_token_id]]\n",
    "    )\n",
    "\n",
    "output = tokenizer.decode(output_ids.tolist()[0])\n",
    "print(postprocess_output(output).split('応答:')[-1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0c34182c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bos_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "・AIとは何か？\n",
      "AI（人工知能）はコンピューターシステムの一種であり、人間の脳を模倣してデータから学習し、パターンやアルゴリズムを作成する技術であると考えられています。機械が人間のように考えることができるようにすることで、より効率的で生産的な作業を行うことができます。また、AIにより生成された予測モデルは、将来の意思決定において重要な役割\n"
     ]
    }
   ],
   "source": [
    "instruction = \"AIについてどう思いますか？\"\n",
    "input_text = NO_INPUT_PROMPT.format(instruction=instruction)\n",
    "token_ids = tokenizer.encode(input_text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(\n",
    "        token_ids.to(model.device),\n",
    "        min_length=32,\n",
    "        max_length=128,\n",
    "        temperature=0.1,\n",
    "        do_sample=True,\n",
    "        no_repeat_ngram_size=3,\n",
    "        repetition_penalty=1.2,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        bad_words_ids=[[tokenizer.unk_token_id]]\n",
    "    )\n",
    "\n",
    "output = tokenizer.decode(output_ids.tolist()[0])\n",
    "print(postprocess_output(output).split('応答:')[-1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d8bc301f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bos_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "例えば、「横須賀市はアメリカ海軍基地があることで有名で、海軍の街として知られています」という意味になります。\n"
     ]
    }
   ],
   "source": [
    "instruction = \"横須賀はどんなところ？\"\n",
    "input_text = NO_INPUT_PROMPT.format(instruction=instruction)\n",
    "token_ids = tokenizer.encode(input_text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(\n",
    "        token_ids.to(model.device),\n",
    "        min_length=32,\n",
    "        max_length=128,\n",
    "        temperature=0.1,\n",
    "        do_sample=True,\n",
    "        no_repeat_ngram_size=3,\n",
    "        repetition_penalty=1.2,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        bad_words_ids=[[tokenizer.unk_token_id]]\n",
    "    )\n",
    "\n",
    "output = tokenizer.decode(output_ids.tolist()[0])\n",
    "print(postprocess_output(output).split('応答:')[-1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "60183462",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bos_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "例えば、横須賀市はアメリカ海軍の基地があることで有名ですが、そのほかにも魅力的な観光スポットやアクティビティがたくさんあります。\n"
     ]
    }
   ],
   "source": [
    "instruction = \"横須賀はどんなところ？\"\n",
    "input_text = NO_INPUT_PROMPT.format(instruction=instruction)\n",
    "token_ids = tokenizer.encode(input_text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(\n",
    "        token_ids.to(model.device),\n",
    "        min_length=32,\n",
    "        max_length=128,\n",
    "        temperature=0.2,\n",
    "        do_sample=True,\n",
    "        no_repeat_ngram_size=3,\n",
    "        repetition_penalty=1.2,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        bad_words_ids=[[tokenizer.unk_token_id]]\n",
    "    )\n",
    "\n",
    "output = tokenizer.decode(output_ids.tolist()[0])\n",
    "print(postprocess_output(output).split('応答:')[-1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e0e73b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bos_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "例えば、横須賀市はアメリカ海軍の基地があることで知られていますが、そのほかにも多くの文化的・歴史的な見どころがあります。\n"
     ]
    }
   ],
   "source": [
    "instruction = \"横須賀はどんなところですか？\"\n",
    "input_text = NO_INPUT_PROMPT.format(instruction=instruction)\n",
    "token_ids = tokenizer.encode(input_text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(\n",
    "        token_ids.to(model.device),\n",
    "        min_length=32,\n",
    "        max_length=128,\n",
    "        temperature=0.2,\n",
    "        do_sample=True,\n",
    "        no_repeat_ngram_size=3,\n",
    "        repetition_penalty=1.2,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        bad_words_ids=[[tokenizer.unk_token_id]]\n",
    "    )\n",
    "\n",
    "output = tokenizer.decode(output_ids.tolist()[0])\n",
    "print(postprocess_output(output).split('応答:')[-1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5143a5a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bos_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "例えば、ドナルド・トランプ大統領がいます。\n"
     ]
    }
   ],
   "source": [
    "instruction = \"今のアメリカの大統領は誰ですか？\"\n",
    "input_text = NO_INPUT_PROMPT.format(instruction=instruction)\n",
    "token_ids = tokenizer.encode(input_text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(\n",
    "        token_ids.to(model.device),\n",
    "        min_length=32,\n",
    "        max_length=128,\n",
    "        temperature=0.1,\n",
    "        do_sample=True,\n",
    "        no_repeat_ngram_size=3,\n",
    "        repetition_penalty=1.2,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        bad_words_ids=[[tokenizer.unk_token_id]]\n",
    "    )\n",
    "\n",
    "output = tokenizer.decode(output_ids.tolist()[0])\n",
    "print(postprocess_output(output).split('応答:')[-1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e0c7b292",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bos_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "例えば、古代ギリシャの哲学者たちは、自然界がどのように機能しているかを理解するために研究しました。彼らは、宇宙には物質的なものとエネルギー的なものの2つの異なる形態があることを発見し、これらの両方が相互作用して調和のとれた状態になる必要があると考えました。\n"
     ]
    }
   ],
   "source": [
    "instruction = \"世界の歴史について教えてください。\"\n",
    "input_text = NO_INPUT_PROMPT.format(instruction=instruction)\n",
    "token_ids = tokenizer.encode(input_text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(\n",
    "        token_ids.to(model.device),\n",
    "        min_length=32,\n",
    "        max_length=128,\n",
    "        temperature=0.1,\n",
    "        do_sample=True,\n",
    "        no_repeat_ngram_size=3,\n",
    "        repetition_penalty=1.2,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        bad_words_ids=[[tokenizer.unk_token_id]]\n",
    "    )\n",
    "\n",
    "output = tokenizer.decode(output_ids.tolist()[0])\n",
    "print(postprocess_output(output).split('応答:')[-1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7335a375",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6d6646c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- instruction -----\n",
      "健康を維持するための3つのヒントを教えてください。\n",
      "\n",
      "----- input -----\n",
      "\n",
      "\n",
      "----- output -----\n",
      "バランスの良い食事を摂り、果物や野菜をたっぷりと含めるようにしてください。\n",
      "2. 定期的に運動して、体を活発で強く保ちましょう。\n",
      "3. 十分な睡眠をとり、一定の睡眠スケジュールを維持してください。\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bos_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. 定期的な運動をすること。2. 十分な睡眠をとること。3. ストレスや不安感に対処すること。\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "instruction = japanese_alpaca_ds['train'][i]['instruction']\n",
    "input_ = japanese_alpaca_ds['train'][i]['input']\n",
    "output = japanese_alpaca_ds['train'][i]['output']\n",
    "print(f\"----- instruction -----\")\n",
    "print(instruction)\n",
    "print()\n",
    "print(f\"----- input -----\")\n",
    "print(input_)\n",
    "print()\n",
    "print(f\"----- output -----\")\n",
    "print(output)\n",
    "print()\n",
    "\n",
    "input_text = NO_INPUT_PROMPT.format(instruction=instruction)\n",
    "token_ids = tokenizer.encode(input_text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(\n",
    "        token_ids.to(model.device),\n",
    "        min_length=32,\n",
    "        max_length=128,\n",
    "        temperature=0.1,\n",
    "        do_sample=True,\n",
    "        no_repeat_ngram_size=3,\n",
    "        repetition_penalty=1.2,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        bad_words_ids=[[tokenizer.unk_token_id]]\n",
    "    )\n",
    "\n",
    "output = tokenizer.decode(output_ids.tolist()[0])\n",
    "print(postprocess_output(output).split('応答:')[-1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e0f1753b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- instruction -----\n",
      "従業員の給与を記録するためのデータベースを設計してください。\n",
      "\n",
      "----- input -----\n",
      "\n",
      "\n",
      "----- output -----\n",
      "データベースには、従業員の名前、役職、給与、および日付のフィールドが含まれている必要があります。また、従業員のマネージャーのフィールドも含まれている必要があります。これにより、給与が部門全体で適切に割り当てられます。データベースは、部門または個人の給与費用に関するレポートを生成できるようにする必要があります。\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bos_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- response -----\n",
      "例えば、「従業員10人の場合」というように、データの種類や数量によって異なる値を提供することができます。\n"
     ]
    }
   ],
   "source": [
    "i = 100\n",
    "instruction = japanese_alpaca_ds['train'][i]['instruction']\n",
    "input_ = japanese_alpaca_ds['train'][i]['input']\n",
    "output = japanese_alpaca_ds['train'][i]['output']\n",
    "print(f\"----- instruction -----\")\n",
    "print(instruction)\n",
    "print()\n",
    "print(f\"----- input -----\")\n",
    "print(input_)\n",
    "print()\n",
    "print(f\"----- output -----\")\n",
    "print(output)\n",
    "print()\n",
    "\n",
    "input_text = NO_INPUT_PROMPT.format(instruction=instruction)\n",
    "token_ids = tokenizer.encode(input_text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(\n",
    "        token_ids.to(model.device),\n",
    "        min_length=32,\n",
    "        max_length=128,\n",
    "        temperature=0.1,\n",
    "        do_sample=True,\n",
    "        no_repeat_ngram_size=3,\n",
    "        repetition_penalty=1.2,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        bad_words_ids=[[tokenizer.unk_token_id]]\n",
    "    )\n",
    "\n",
    "output = tokenizer.decode(output_ids.tolist()[0])\n",
    "response = postprocess_output(output).split('応答:')[-1].strip()\n",
    "print(\"----- response -----\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a7e6241e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bos_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- instruction -----\n",
      "以下の国に関連する歴史的な出来事を3つ挙げてください\n",
      "\n",
      "----- input -----\n",
      "カナダ\n",
      "\n",
      "----- output -----\n",
      "カナダに関連する歴史的な3つの出来事は、1867年のカナダ連邦の設立、1965年のカナダ国旗の採用、そして1988年のカナダ・アメリカ自由貿易協定の署名です。\n",
      "\n",
      "----- response -----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i = 1000\n",
    "instruction = japanese_alpaca_ds['train'][i]['instruction']\n",
    "input_ = japanese_alpaca_ds['train'][i]['input']\n",
    "output = japanese_alpaca_ds['train'][i]['output']\n",
    "print(f\"----- instruction -----\")\n",
    "print(instruction)\n",
    "print()\n",
    "print(f\"----- input -----\")\n",
    "print(input_)\n",
    "print()\n",
    "print(f\"----- output -----\")\n",
    "print(output)\n",
    "print()\n",
    "\n",
    "input_text = NO_INPUT_PROMPT.format(instruction=instruction)\n",
    "token_ids = tokenizer.encode(input_text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(\n",
    "        token_ids.to(model.device),\n",
    "        min_length=32,\n",
    "        max_length=128,\n",
    "        temperature=0.1,\n",
    "        do_sample=True,\n",
    "        no_repeat_ngram_size=3,\n",
    "        repetition_penalty=1.2,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        bad_words_ids=[[tokenizer.unk_token_id]]\n",
    "    )\n",
    "\n",
    "output = tokenizer.decode(output_ids.tolist()[0])\n",
    "response = postprocess_output(output).split('応答:')[-1].strip()\n",
    "print(\"----- response -----\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bd9f196f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- instruction -----\n",
      "以下の国に関連する歴史的な出来事を3つ挙げてください\n",
      "\n",
      "----- input -----\n",
      "カナダ\n",
      "\n",
      "----- output -----\n",
      "カナダに関連する歴史的な3つの出来事は、1867年のカナダ連邦の設立、1965年のカナダ国旗の採用、そして1988年のカナダ・アメリカ自由貿易協定の署名です。\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bos_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- response -----\n",
      "1. 古代ローマ\n",
      "2. 古代ギリシャ\n",
      "3. 古代エジプト\n"
     ]
    }
   ],
   "source": [
    "i = 1000\n",
    "instruction = japanese_alpaca_ds['train'][i]['instruction']\n",
    "input_ = japanese_alpaca_ds['train'][i]['input']\n",
    "output = japanese_alpaca_ds['train'][i]['output']\n",
    "print(f\"----- instruction -----\")\n",
    "print(instruction)\n",
    "print()\n",
    "print(f\"----- input -----\")\n",
    "print(input_)\n",
    "print()\n",
    "print(f\"----- output -----\")\n",
    "print(output)\n",
    "print()\n",
    "\n",
    "input_text = NO_INPUT_PROMPT.format(instruction=instruction)\n",
    "token_ids = tokenizer.encode(input_text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(\n",
    "        token_ids.to(model.device),\n",
    "        min_length=32,\n",
    "        max_length=128,\n",
    "        temperature=0.1,\n",
    "        do_sample=True,\n",
    "#         no_repeat_ngram_size=3,\n",
    "#         repetition_penalty=1.2,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        bad_words_ids=[[tokenizer.unk_token_id]]\n",
    "    )\n",
    "\n",
    "output = tokenizer.decode(output_ids.tolist()[0])\n",
    "response = postprocess_output(output).split('応答:')[-1].strip()\n",
    "print(\"----- response -----\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "751bea11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- instruction -----\n",
      "\"フードデザート\"とは何か、そしてなぜそれらが問題なのかを説明してください。\n",
      "\n",
      "----- input -----\n",
      "\n",
      "\n",
      "----- output -----\n",
      "\"フードデザート\"とは、手頃な価格で健康的な食品がほとんどない地域のことを指します。通常、高い貧困率とスーパーマーケットや健康的な食品の供給源が少ない都市部にあります。フードデザートは、新鮮で栄養価の高い食品にアクセスできないため、肥満、糖尿病、栄養失調などの健康問題を引き起こします。\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bos_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- response -----\n",
      "フードデザートは、食品のデザート、またはデザートの一種です。通常、果物や野菜、または穀物などの食品に、砂糖、シロップ、またはクリームなどのトッピングを加えることで作られます。\n"
     ]
    }
   ],
   "source": [
    "i = 2000\n",
    "instruction = japanese_alpaca_ds['train'][i]['instruction']\n",
    "input_ = japanese_alpaca_ds['train'][i]['input']\n",
    "output = japanese_alpaca_ds['train'][i]['output']\n",
    "print(f\"----- instruction -----\")\n",
    "print(instruction)\n",
    "print()\n",
    "print(f\"----- input -----\")\n",
    "print(input_)\n",
    "print()\n",
    "print(f\"----- output -----\")\n",
    "print(output)\n",
    "print()\n",
    "\n",
    "input_text = NO_INPUT_PROMPT.format(instruction=instruction)\n",
    "token_ids = tokenizer.encode(input_text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(\n",
    "        token_ids.to(model.device),\n",
    "        min_length=32,\n",
    "        max_length=128,\n",
    "        temperature=0.1,\n",
    "        do_sample=True,\n",
    "#         no_repeat_ngram_size=3,\n",
    "#         repetition_penalty=1.2,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        bad_words_ids=[[tokenizer.unk_token_id]]\n",
    "    )\n",
    "\n",
    "output = tokenizer.decode(output_ids.tolist()[0])\n",
    "response = postprocess_output(output).split('応答:')[-1].strip()\n",
    "print(\"----- response -----\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "474f6ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bos_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- response -----\n",
      "横須賀は、日本海軍の拠点として発展した歴史を持ち、海上自衛隊や海上保安庁の基地が置かれています。また、軍港として発展し、海上自衛隊の基地や海上自衛隊が保有する艦艇が配備されています。また、「日本の空の玄関口」として、航空自衛隊や航空自衛隊の航空基地もあります。\n"
     ]
    }
   ],
   "source": [
    "instruction = \"横須賀はどんなところ？\"\n",
    "\n",
    "input_text = NO_INPUT_PROMPT.format(instruction=instruction)\n",
    "token_ids = tokenizer.encode(input_text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(\n",
    "        token_ids.to(model.device),\n",
    "        min_length=32,\n",
    "        max_length=128,\n",
    "        temperature=0.1,\n",
    "        do_sample=True,\n",
    "        no_repeat_ngram_size=3,\n",
    "#         repetition_penalty=1.2,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        bad_words_ids=[[tokenizer.unk_token_id]]\n",
    "    )\n",
    "\n",
    "output = tokenizer.decode(output_ids.tolist()[0])\n",
    "response = postprocess_output(output).split('応答:')[-1].strip()\n",
    "print(\"----- response -----\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8953ca56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bos_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- response -----\n",
      "横須賀は、日本海軍の拠点として、日本海軍の重要な拠点でした。第二次世界大戦の終結後、日本海軍は、横須賀を拠点として、日本海軍の重要な拠点となりました。第二次世界大戦の終結後、日本海軍は、横須賀を拠点として、日本海軍の重要な拠点となりました。\n"
     ]
    }
   ],
   "source": [
    "instruction = \"横須賀はどんなところ？\"\n",
    "\n",
    "input_text = NO_INPUT_PROMPT.format(instruction=instruction)\n",
    "token_ids = tokenizer.encode(input_text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(\n",
    "        token_ids.to(model.device),\n",
    "        min_length=32,\n",
    "        max_length=128,\n",
    "        temperature=0.1,\n",
    "        do_sample=True,\n",
    "#         no_repeat_ngram_size=3,\n",
    "#         repetition_penalty=1.2,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        bad_words_ids=[[tokenizer.unk_token_id]]\n",
    "    )\n",
    "\n",
    "output = tokenizer.decode(output_ids.tolist()[0])\n",
    "response = postprocess_output(output).split('応答:')[-1].strip()\n",
    "print(\"----- response -----\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3f41338b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bos_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- response -----\n",
      "横須賀市は東京都心から約1時間半の距離にあり、東京湾沿いの美しい海岸線や豊かな自然環境に囲まれています。また、日本最大の軍港である浦賀水道もあります。\n"
     ]
    }
   ],
   "source": [
    "instruction = \"横須賀はどんなところ？\"\n",
    "\n",
    "input_text = NO_INPUT_PROMPT.format(instruction=instruction)\n",
    "token_ids = tokenizer.encode(input_text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(\n",
    "        token_ids.to(model.device),\n",
    "        min_length=32,\n",
    "        max_length=128,\n",
    "        temperature=0.1,\n",
    "        do_sample=True,\n",
    "#         no_repeat_ngram_size=3,\n",
    "        repetition_penalty=1.2,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        bad_words_ids=[[tokenizer.unk_token_id]]\n",
    "    )\n",
    "\n",
    "output = tokenizer.decode(output_ids.tolist()[0])\n",
    "response = postprocess_output(output).split('応答:')[-1].strip()\n",
    "print(\"----- response -----\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a14623bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bos_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- response -----\n",
      "明日の天気は晴れです。\n"
     ]
    }
   ],
   "source": [
    "instruction = \"明日の天気は？\"\n",
    "\n",
    "input_text = NO_INPUT_PROMPT.format(instruction=instruction)\n",
    "token_ids = tokenizer.encode(input_text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(\n",
    "        token_ids.to(model.device),\n",
    "        min_length=32,\n",
    "        max_length=128,\n",
    "        temperature=0.1,\n",
    "        do_sample=True,\n",
    "#         no_repeat_ngram_size=3,\n",
    "#         repetition_penalty=1.2,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        bad_words_ids=[[tokenizer.unk_token_id]]\n",
    "    )\n",
    "\n",
    "output = tokenizer.decode(output_ids.tolist()[0])\n",
    "response = postprocess_output(output).split('応答:')[-1].strip()\n",
    "print(\"----- response -----\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ec0bdc4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bos_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- response -----\n",
      "明日は雨になるでしょう！\n"
     ]
    }
   ],
   "source": [
    "instruction = \"明日の天気は？\"\n",
    "\n",
    "input_text = NO_INPUT_PROMPT.format(instruction=instruction)\n",
    "token_ids = tokenizer.encode(input_text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(\n",
    "        token_ids.to(model.device),\n",
    "        min_length=32,\n",
    "        max_length=128,\n",
    "        temperature=0.1,\n",
    "        do_sample=True,\n",
    "#         no_repeat_ngram_size=3,\n",
    "        repetition_penalty=1.2,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        bad_words_ids=[[tokenizer.unk_token_id]]\n",
    "    )\n",
    "\n",
    "output = tokenizer.decode(output_ids.tolist()[0])\n",
    "response = postprocess_output(output).split('応答:')[-1].strip()\n",
    "print(\"----- response -----\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "22e5eab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bos_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- response -----\n",
      "明日は晴れで気温が20度を超えるでしょう！\n"
     ]
    }
   ],
   "source": [
    "instruction = \"明日の天気は？\"\n",
    "\n",
    "input_text = NO_INPUT_PROMPT.format(instruction=instruction)\n",
    "token_ids = tokenizer.encode(input_text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(\n",
    "        token_ids.to(model.device),\n",
    "        min_length=32,\n",
    "        max_length=128,\n",
    "        temperature=0.1,\n",
    "        do_sample=True,\n",
    "#         no_repeat_ngram_size=3,\n",
    "        repetition_penalty=1.2,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        bad_words_ids=[[tokenizer.unk_token_id]]\n",
    "    )\n",
    "\n",
    "output = tokenizer.decode(output_ids.tolist()[0])\n",
    "response = postprocess_output(output).split('応答:')[-1].strip()\n",
    "print(\"----- response -----\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ebcf9efe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bos_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- response -----\n",
      "私は肉料理が特に好きです！\n"
     ]
    }
   ],
   "source": [
    "instruction = \"どんな料理が好き？\"\n",
    "\n",
    "input_text = NO_INPUT_PROMPT.format(instruction=instruction)\n",
    "token_ids = tokenizer.encode(input_text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(\n",
    "        token_ids.to(model.device),\n",
    "        min_length=32,\n",
    "        max_length=128,\n",
    "        temperature=0.1,\n",
    "        do_sample=True,\n",
    "#         no_repeat_ngram_size=3,\n",
    "        repetition_penalty=1.2,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        bad_words_ids=[[tokenizer.unk_token_id]]\n",
    "    )\n",
    "\n",
    "output = tokenizer.decode(output_ids.tolist()[0])\n",
    "response = postprocess_output(output).split('応答:')[-1].strip()\n",
    "print(\"----- response -----\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "374c5672",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bos_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- response -----\n",
      "現在の日本国の首相は安倍晋三さんです。\n"
     ]
    }
   ],
   "source": [
    "instruction = \"今の日本の総理大臣は誰か知っていますか？\"\n",
    "\n",
    "input_text = NO_INPUT_PROMPT.format(instruction=instruction)\n",
    "token_ids = tokenizer.encode(input_text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(\n",
    "        token_ids.to(model.device),\n",
    "        min_length=32,\n",
    "        max_length=128,\n",
    "        temperature=0.1,\n",
    "        do_sample=True,\n",
    "#         no_repeat_ngram_size=3,\n",
    "        repetition_penalty=1.2,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        bad_words_ids=[[tokenizer.unk_token_id]]\n",
    "    )\n",
    "\n",
    "output = tokenizer.decode(output_ids.tolist()[0])\n",
    "response = postprocess_output(output).split('応答:')[-1].strip()\n",
    "print(\"----- response -----\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "22ab4954",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bos_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "東京\n"
     ]
    }
   ],
   "source": [
    "instruction = \"日本の首都は？\"\n",
    "input_text = NO_INPUT_PROMPT.format(instruction=instruction)\n",
    "token_ids = tokenizer.encode(input_text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(\n",
    "        token_ids.to(model.device),\n",
    "        min_length=32,\n",
    "        max_length=128,\n",
    "        temperature=0.1,\n",
    "        do_sample=True,\n",
    "        # no_repeat_ngram_size=3,\n",
    "        repetition_penalty=1.2,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        bad_words_ids=[[tokenizer.unk_token_id]]\n",
    "    )\n",
    "\n",
    "output = tokenizer.decode(output_ids.tolist()[0])\n",
    "print(postprocess_output(output).split('応答:')[-1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c2a36ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bos_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "フランス共和国の首都はパリで、人口は約105万人です。\n"
     ]
    }
   ],
   "source": [
    "instruction = \"フランスの首都は？\"\n",
    "input_text = NO_INPUT_PROMPT.format(instruction=instruction)\n",
    "token_ids = tokenizer.encode(input_text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(\n",
    "        token_ids.to(model.device),\n",
    "        min_length=32,\n",
    "        max_length=128,\n",
    "        temperature=0.1,\n",
    "        do_sample=True,\n",
    "        # no_repeat_ngram_size=3,\n",
    "        repetition_penalty=1.2,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        bad_words_ids=[[tokenizer.unk_token_id]]\n",
    "    )\n",
    "\n",
    "output = tokenizer.decode(output_ids.tolist()[0])\n",
    "print(postprocess_output(output).split('応答:')[-1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d403a45e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bos_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "フランスの中央に位置する都市はパリで、人口は約140万人です。\n"
     ]
    }
   ],
   "source": [
    "instruction = \"フランスの首都は？\"\n",
    "input_text = NO_INPUT_PROMPT.format(instruction=instruction)\n",
    "token_ids = tokenizer.encode(input_text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(\n",
    "        token_ids.to(model.device),\n",
    "        min_length=32,\n",
    "        max_length=128,\n",
    "        temperature=0.1,\n",
    "        do_sample=True,\n",
    "        # no_repeat_ngram_size=3,\n",
    "        repetition_penalty=1.2,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        bad_words_ids=[[tokenizer.unk_token_id]]\n",
    "    )\n",
    "\n",
    "output = tokenizer.decode(output_ids.tolist()[0])\n",
    "print(postprocess_output(output).split('応答:')[-1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1462638c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bos_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ドイツ連邦共和国にある都市の一覧は以下の通りです：ベルリン、ドレスデン、ケルン、デュッセルドルフ、ミュンヘン\n"
     ]
    }
   ],
   "source": [
    "instruction = \"ドイツの首都は？\"\n",
    "input_text = NO_INPUT_PROMPT.format(instruction=instruction)\n",
    "token_ids = tokenizer.encode(input_text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(\n",
    "        token_ids.to(model.device),\n",
    "        min_length=32,\n",
    "        max_length=128,\n",
    "        temperature=0.1,\n",
    "        do_sample=True,\n",
    "        # no_repeat_ngram_size=3,\n",
    "        repetition_penalty=1.2,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        bad_words_ids=[[tokenizer.unk_token_id]]\n",
    "    )\n",
    "\n",
    "output = tokenizer.decode(output_ids.tolist()[0])\n",
    "print(postprocess_output(output).split('応答:')[-1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3792275a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bos_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "渋谷には美味しいケーキ屋さんがたくさんあります！\n"
     ]
    }
   ],
   "source": [
    "instruction = \"渋谷のおすすめのスイーツを教えて。\"\n",
    "input_text = NO_INPUT_PROMPT.format(instruction=instruction)\n",
    "token_ids = tokenizer.encode(input_text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(\n",
    "        token_ids.to(model.device),\n",
    "        min_length=32,\n",
    "        max_length=128,\n",
    "        temperature=0.1,\n",
    "        do_sample=True,\n",
    "        # no_repeat_ngram_size=3,\n",
    "        repetition_penalty=1.2,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        bad_words_ids=[[tokenizer.unk_token_id]]\n",
    "    )\n",
    "\n",
    "output = tokenizer.decode(output_ids.tolist()[0])\n",
    "print(postprocess_output(output).split('応答:')[-1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "562e4853",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bos_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "日本には多くの見所がありますが、最も人気があるのは富士山や京都など自然の美しさを楽しむことができる場所でしょう。また、温泉も人気があります。\n"
     ]
    }
   ],
   "source": [
    "instruction = \"日本のおすすめの観光スポットは？\"\n",
    "input_text = NO_INPUT_PROMPT.format(instruction=instruction)\n",
    "token_ids = tokenizer.encode(input_text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(\n",
    "        token_ids.to(model.device),\n",
    "        min_length=32,\n",
    "        max_length=128,\n",
    "        temperature=0.1,\n",
    "        do_sample=True,\n",
    "        # no_repeat_ngram_size=3,\n",
    "        repetition_penalty=1.2,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        bad_words_ids=[[tokenizer.unk_token_id]]\n",
    "    )\n",
    "\n",
    "output = tokenizer.decode(output_ids.tolist()[0])\n",
    "print(postprocess_output(output).split('応答:')[-1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc604b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
